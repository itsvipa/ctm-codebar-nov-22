{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "iE-ghUvQvk7y",
        "cK_RpyVP-TWZ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itsvipa/ctm-codebar-nov-22/blob/main/codebar_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code for Living (Python for Data Engineering) Solutions"
      ],
      "metadata": {
        "id": "NalAJsHGvWab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this session we’ll be taking a look at harnessing the power of data to aid decision making for things like groceries and everyday purchases. We’ll get hands on with sourcing and making sense of the data, and then using it to answer some interesting questions, using techniques from fields like optimisation and machine learning."
      ],
      "metadata": {
        "id": "JgRY-jJxvckc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tasks:\n",
        "1. Find the cheapest supermarket for a given shopping basket\n",
        "2. Find out if you can save money by shopping at multiple stores.\n",
        "3. Use nutritional information to find healthy, cheap alternatives (Optional)\n",
        "4. Categorize the products and compare them using machine learning (Optional)"
      ],
      "metadata": {
        "id": "ivk62-P3NMYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Before you start:\n",
        "Run the command below to download the data needed for this exercise. There should be three .csv files downloaded: \n",
        "\n",
        "*   products.csv\n",
        "*   prices.csv\n",
        "*   nutrition_small.csv.gz\n",
        "\n",
        "NOTE: A command is a way for users to send \n",
        "instructions to the computer for it to process. In this case, the command is asking the `git` tool to copy the files from this location in github into this notebook environment."
      ],
      "metadata": {
        "id": "1Ff1H7KJwXBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/itsvipa/ctm-codebar-nov-22.git"
      ],
      "metadata": {
        "id": "eQTfIYOryNPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1 - Find the cheapest supermarket for your groceries\n",
        "\n",
        "First, import pandas into your program:"
      ],
      "metadata": {
        "id": "iE-ghUvQvk7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "0vST9NFmwH2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pandas is a python library that is used for data manipulation and analysis by data into DataFrames this data can be in the form of dictionaries, TSVs CSVs and more. More info on Pandas can be found at https://pandas.pydata.org/ and https://realpython.com/pandas-read-write-files/ \n",
        "\n",
        "\n",
        "For this exercise, we are going to use the following list of items - match sure to run the cell below and load them in!"
      ],
      "metadata": {
        "id": "4O8JTpymT-kU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "basket = [\n",
        "    [28798, \"Tetley Original Tea Bags x240\"], \n",
        "    [34398, \"Cravendale Whole Milk\"],\n",
        "    [30904, \"Kellogg's Rice Krispies\"],\n",
        "    [26457,\t\"Tropicana Pineapple Juice\"],\n",
        "    [28095, \"Diet Coke Bottles\"],\n",
        "    [33442, \"Loyd Grossman Tomato & Basil Pasta Sauce\"],\n",
        "    [27161,\t\"Strings & Things Cheestrings Cheese Snack\"],\n",
        "    [31863,\t\"Mars 4 pack Ice Cream Bars\"],\n",
        "    [32002,\t\"Tilda Microwave Coconut Basmati Rice\"],\n",
        "    [86547,\t\"Knorr Chicken Stock Pot 4 Pack\"],\n",
        "    [84095,\t\"Arla skyr Fat Free Natural Yogurt\"],\n",
        "    [33255,\t\"Alpro Almond Long Life Drink\"],\n",
        "    [43809,\t\"Energizer Max 9V Batteries\"],\n",
        "    [25828,\t\"Robinsons Crushed Lime & Mint Fruit Cordial\"],\n",
        "    [32248,\t\"Sensations Balsamic Vinegar & Caramelised Onion Sharing Crisps\"],\n",
        "    [47361,\t\"Lynx Africa Body Spray Deodorant Aerosol\"],\n",
        "    [32774,\t\"Heinz Spaghetti\"],\n",
        "    [92101,\t\"Chicago Town Cheese Pizza Tomato Stuffed Crust Takeaway\"],\n",
        "    [110105, \"McCain Home Chips Straight\"],\n",
        "    [44797,\t\"Fairy Original Washing Up Liquid\"],\n",
        "    [40915, \"Fairy Non Bio Washing Liquid for Sensitive Skin 1.33L 38 Washes\"],\n",
        "    [24091,\t\"Nissin Soba Yakitori Noodles\"],\n",
        "    [42002,\t\"Yakult Yogurt Drink\"],\t\n",
        "    [83092,\t\"Jammie Dodgers\"],\n",
        "    [20272,\t\"Quorn Vegan Chicken Free Slices 100G\"],\t\n",
        "    [33492,\t\"Linda McCartney's Mozzarella Quarter Pounder Burger\"],\n",
        "    [51657,\t\"Purina ONE Adult Dry Cat Food Chicken & Wholegrains\"],\n",
        "    [24341,\t\"Birds Eye 4 Original Beef Burgers With Onions\"],\n",
        "    [109986, \"Cathedral City Lighter Mature Cheese\"],\n",
        "    [27526,\t\"Filippo Berio Mild & Light Olive Oil\"],\n",
        "    [45238, \"Bfree Pitta Breads Stone Baked Pittas\"]\n",
        "    ]\n",
        "\n",
        "print(len(basket))"
      ],
      "metadata": {
        "id": "bM1jXslF--ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the data \n",
        "\n",
        "The first step to get price create two DataFrames for the product and price.\n",
        "\n",
        "`product.csv` is list of all the products we found on https://priceable.co.uk/, and should contain information on the product name, the quantity, the product id etc\n",
        "\n",
        "`price.csv` is a list of the prices of the products above at different supermarkets, from the same website, and should contain infomation on the product price, the supermarket name, the product name etc\n",
        "\n",
        "A pandas DataFrame is a nice easy way of storing table data in Python.\n",
        "\n",
        "Using the `pd.read_csv` method will take a csv and put it into a pandas DataFrame for us to use.\n",
        "\n"
      ],
      "metadata": {
        "id": "x-hRRrd1UCUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "product_df = pd.read_csv(\"/content/ctm-codebar-nov-22/products.csv\")\n",
        "price_df = pd.read_csv(\"/content/ctm-codebar-nov-22/prices.csv\")"
      ],
      "metadata": {
        "id": "0NAOcdiTQZ9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To show what the DataFrame actually looks like, we can just call the dataframe, like below"
      ],
      "metadata": {
        "id": "MntNAmXAULn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "product_df"
      ],
      "metadata": {
        "id": "Tu-tMTxjQoEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try and call the price_df DataFrame and have a look at some of the data. Can you see any similaries between the two?"
      ],
      "metadata": {
        "id": "GfUZA_lMUQka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#call the price_df\n",
        "price_df"
      ],
      "metadata": {
        "id": "vgMIdVt5Qryo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the basket ids\n",
        "\n",
        "The id column for both prices and product are the exact same. This means that the ids can be used to find the products and their price, within the price dataset. To do this, first a list of the ids for the basket is needed"
      ],
      "metadata": {
        "id": "0WXQuyV0UTBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a for loop, get a list of the product ids from the basket object"
      ],
      "metadata": {
        "id": "pAgWjrTlUbln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of the basket ids\n",
        "basket_ids = [i[0] for i in basket]"
      ],
      "metadata": {
        "id": "Q1k1V2Y9oDt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Return the basket ids to check that the list is correct"
      ],
      "metadata": {
        "id": "ord61elDUliR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Return the list of basket ids\n",
        "basket_ids"
      ],
      "metadata": {
        "id": "8otiQxrgUmz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to figure out which rows in the product dataframe having matching ids to the list we just made. We are going to do this using a method called `.isin()`, which looks at each row, and returns `True` or `False` based on whether the row is in a given list."
      ],
      "metadata": {
        "id": "X31TjSNF-kXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This creates a series of true/false values\n",
        "product_match = product_df['id'].isin(basket_ids)\n",
        "product_match"
      ],
      "metadata": {
        "id": "2-Jeyaep_WLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A cool feature of pandas is that you can take a series of boolean (True/False) values like the one we made above (`product_match`) and use it to filter tables, by putting it within the square brackets like below:"
      ],
      "metadata": {
        "id": "pY5T3E1kUumU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "product_basket = product_df[product_match]"
      ],
      "metadata": {
        "id": "B0-aFZR3U8Q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What this does, is go through the series and table rows together, side by side, and one by one:\n",
        "- keeps the row if it has have value `True` in the series.\n",
        "- hides the row if it have value `False` in the series.\n",
        "\n",
        "To make sure it does this correctly, the number of table rows and number of values in the series need to be the same (which is usually true, when we make the Series directly from the table like above).\n",
        "\n",
        "You can see the result of this below:"
      ],
      "metadata": {
        "id": "wP-yReSKAEqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "product_basket"
      ],
      "metadata": {
        "id": "Be_xIuTzU9Hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtering the data to find the cheapest supermarket"
      ],
      "metadata": {
        "id": "lxenchNDVBdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To filter a DataFrame in pandas you can attach a bracket to you DataFrame, then and your filtering parameters inside. In the example above, we have stated that the new DataFrame product_basket, should be made using data from the product_df, but only if the basket_ids, are in the column `id`"
      ],
      "metadata": {
        "id": "O1F-FEnnVC2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the example above, create a new DataFrame that only contains the prices of products, which match the basket_ids list"
      ],
      "metadata": {
        "id": "PgBibfgiVaHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a filtered DataFrame that only includes information on items in the basket\n",
        "basket_prices = price_df[price_df['id'].isin(basket_ids)]"
      ],
      "metadata": {
        "id": "Iv7voOku8yBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Return the new DataFrame to check it is correct"
      ],
      "metadata": {
        "id": "Y1CjjCgeVTqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Return the DataFrame\n",
        "basket_prices"
      ],
      "metadata": {
        "id": "n_yWISFkVWCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice in the DataFrame above, there is a column called `retailer` with all the supermarket names. We can use a pandas method called `.unique()` to get a list of all the supermarkets "
      ],
      "metadata": {
        "id": "9Y_ezqdqVeAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "basket_prices['retailer'].unique()"
      ],
      "metadata": {
        "id": "LWjpy3ccEmVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When filtering DataFrames in pandas, you can attach methods such as `.unique()` and `.count()` to manipulate the data further"
      ],
      "metadata": {
        "id": "WB9uoY9cVjlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another method within pandas that can be useful is `.group_by()`. `.group_by()` is used to split the data into groups, where each member of the group has the same value for one (or more) of the columns that you pick.\n",
        "\n",
        "To check that the retailers have all the products in our basket, we can use `group_by()` and `.count()` to group the data by the supermarket name, then count the number of products that supermarket has."
      ],
      "metadata": {
        "id": "WjOaTqcfVkWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the basket prices by `retailer` and count how many of the items each supermarket has \n",
        "basket_prices.groupby(['retailer']).count()"
      ],
      "metadata": {
        "id": "oQYAdqTJwLlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the data above, is there any supermarket we should not include in our analysis? Why? \n",
        "\n",
        "there should be one supermarket, that does not have reliable data, and therefore, can be excluded from the comparison."
      ],
      "metadata": {
        "id": "erEW-Fy_Vt5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, create a list of the retailers that we DO want to test:"
      ],
      "metadata": {
        "id": "7wWzmG1BVvLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create list of retailers we want to compare\n",
        "retailers_to_test = ['morrisons', 'asda', 'coop', 'ocado', 'sainsburys', 'tesco', 'waitrose']"
      ],
      "metadata": {
        "id": "iutnDLiuwsfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then using the `basket_prices` DataFrame made previously, create a new DataFrame that only has the retailers we are testing"
      ],
      "metadata": {
        "id": "NiTR1EYGV2GK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Created filtered DataFrame that only has basket prices for retailers we are testing\n",
        "compare_price_basket = basket_prices[basket_prices['retailer'].isin(retailers_to_test)]"
      ],
      "metadata": {
        "id": "JHGf7zkNVzY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_price_basket"
      ],
      "metadata": {
        "id": "Uu4AuLQ_V-XC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have a dataframe that contains all the products, the supermarkets and the price, we can do one more aggregation to find the cheapest supermarket.\n",
        "\n",
        "Try grouping the data in compare_price_basket by `retailer`, and adding the price of the items in that supermarket, to find the cheapest place"
      ],
      "metadata": {
        "id": "rzJNgRg9V-FF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group data and add prices of items for each retailer to see which is the cheapest overall\n",
        "compare_price_basket.groupby('retailer').sum()['price']"
      ],
      "metadata": {
        "id": "NYGSrxqZxFEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which supermarket was the cheapest? Are you suprised by the result??"
      ],
      "metadata": {
        "id": "HV2L30SPWIz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task 2 - Shopping at Multiple Stores"
      ],
      "metadata": {
        "id": "cK_RpyVP-TWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we know the cheapest supermarket overall, what happens if we decide to do our shopping in multiple supermarkets? How much money would we save if we split our shopping over more than one store?\n",
        "\n",
        "###Store Combinations\n",
        "\n",
        "To do this, we first need to figure all the different combinations of stores we can shop at, so we can compare the combinations.\n",
        "\n",
        "A tool that will make our lives a lot easier is a module you can import called [itertools](https://docs.python.org/3/library/itertools.html).\n",
        "\n",
        "First lets start by importing the module:"
      ],
      "metadata": {
        "id": "ereEdIjkDwsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools"
      ],
      "metadata": {
        "id": "Ep0Bh4dNWgvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One useful method from itertools that can be used to to this is [`.combinations()`](https://www.geeksforgeeks.org/python-itertools-combinations-function/). \n",
        "\n",
        "`.combinations()` takes in two arguments: \n",
        "\n",
        "\n",
        "1.   The iterator - In this case, that is the `retailers_to_test` list\n",
        "2.   `r` - the size of the combinations -  2 will create pairs, 3 will create sets of 3 etc."
      ],
      "metadata": {
        "id": "mHvjrCYbWiKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using `combinations()` can you create a list of the potential pairs of supermarkets?"
      ],
      "metadata": {
        "id": "qNe2jsZlWlZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of store combinations\n",
        "retailer_pairs = [pairs for pairs in itertools.combinations(retailers_to_test, 2)]"
      ],
      "metadata": {
        "id": "ZJH9v1EnWo9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print out the list here to see if it works as expected.\n",
        "retailer_pairs"
      ],
      "metadata": {
        "id": "0vM-cn-4Wt5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have the potential pairs, its time to find the total of the basket if you were to shop at each pair of supermarkets.\n",
        "\n",
        "We can do this by filtering the prices to only include prices from the 2 selected supermarkets, and finding the cheapest price."
      ],
      "metadata": {
        "id": "_5dQbOdvWyfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here, we are creating a function we can use later to work out cheapest basket\n",
        "# for a given list of retailers\n",
        "def get_cheapest_basket(retailer_list):\n",
        "  '''\n",
        "  Take a list of retailers, and return the total value of the cheapest basket,\n",
        "  if we are only allowed to shop at those retailers\n",
        "  '''\n",
        "\n",
        "  #Using the compare_price_basket we made in task 1, keep only the supermarkets in the list.\n",
        "  retailer_price_basket = compare_price_basket[compare_price_basket['retailer'].isin(retailer_list)]\n",
        "\n",
        "  #Sort by price, keep only the cheapest price for each item id, and sum the prices all together.\n",
        "  return retailer_price_basket.sort_values('price').drop_duplicates(['id'])['price'].sum()\n",
        "\n",
        "pair_basket_prices = [[r, get_cheapest_basket(r)] for r in retailer_pairs]\n",
        "pair_basket_prices"
      ],
      "metadata": {
        "id": "pK5AKw4Mv_6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have a list of lists that contain the store combinations and the total price of the shopping at only these supermarkets. Next we need to sort through this list and find the cheapest overall combination.\n",
        "\n",
        "To do this, we can write a function that searches through each combination and compares the price to the one before. If it is cheaper, than that combination is stored. If not, then it is discarded, and the next combination is checked."
      ],
      "metadata": {
        "id": "XVnIeEMMeyTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a function that gets the cheapest store combination HINT: The function will need a for loop. \n",
        "\n",
        "def get_cheapest_store_combination(num_of_stores):\n",
        "  '''\n",
        "  This function should use the num_of_stores, which tells you how many stores\n",
        "  should be in each combination, and return 2 things:\n",
        "  - The cheapest store combination\n",
        "  - The basket price if we only shopped at those stores\n",
        "  '''\n",
        "  retailer_combinations = itertools.combinations(retailers_to_test, num_of_stores)\n",
        "  cheapest_stores = None\n",
        "  cheapest_price = None\n",
        "  for rc in retailer_combinations:\n",
        "    current_price = get_cheapest_basket(rc)\n",
        "    if not cheapest_price or current_price < cheapest_price:\n",
        "      cheapest_stores = rc\n",
        "      cheapest_price = current_price\n",
        "  return cheapest_stores, cheapest_price"
      ],
      "metadata": {
        "id": "BajmVZXm3wx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can check the cheapest overall combination for 2 supermarkets"
      ],
      "metadata": {
        "id": "p2tEMJOpfwv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_cheapest_store_combination(2)"
      ],
      "metadata": {
        "id": "7LkyYaX2GsgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And even 3..."
      ],
      "metadata": {
        "id": "35rN2n36f4fH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_cheapest_store_combination(3)"
      ],
      "metadata": {
        "id": "6KkU4tGW5iIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also plot this data for all possible number of stores, to better compare the different sized combinations.\n",
        "\n",
        "For the x axis, lets use the range of possible combination sizes from 1 (shopping at one store) to 7 (shopping at 7 stores)."
      ],
      "metadata": {
        "id": "K9HznHJ5gBD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In python, the last number isn't actually included in the range, it includes\n",
        "# the number just before.\n",
        "x_data = range(1,8)"
      ],
      "metadata": {
        "id": "r8IDl5_G528n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, for each value in `x_data`, lets work out the cheapest possible total basket price for shopping at x number of stores."
      ],
      "metadata": {
        "id": "beGiaADhKP6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a list of cheapest prices for each value of x_data\n",
        "y_data = [get_cheapest_store_combination(num_of_stores)[1] for num_of_stores in x_data]\n",
        "\n",
        "y_data"
      ],
      "metadata": {
        "id": "p9vpJWOv6_d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A useful library for plotting graphs in python is matplotlib. More info on this library can be found [here](https://matplotlib.org/)\n",
        "\n",
        "Lets use it below to plot the data we just found."
      ],
      "metadata": {
        "id": "oh1CXUBxhB13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(x_data, y_data)\n",
        "plt.xlabel('Number of Stores')\n",
        "plt.ylabel('Cheapest price for stores')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EFbT7xgN6Qpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a look at the graph. How useful is shopping at different stores?"
      ],
      "metadata": {
        "id": "H4uHRsvjhTpY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3 (Optional) - Using Nutritional Information"
      ],
      "metadata": {
        "id": "un0gzxkBHOUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we've found that some decent savings can be made by just changing where you shop! However, if you are sticking to the same items, there is a limit to how much you can save.\n",
        "\n",
        "This task will take a look at the food you pick, and see how much you can save by changing what you eat (whilst still getting all the nutrition you need!)"
      ],
      "metadata": {
        "id": "nPyieta48MHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Loading the data\n",
        "First, lets load in and take a look at a new dataset we are introducing, a table of nutritional information on a wide selection of items. \n",
        "\n",
        "This dataset was sourced from https://world.openfoodfacts.org/"
      ],
      "metadata": {
        "id": "QQI3MxE4r8qA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nutrition_df = pd.read_csv('/content/ctm-codebar-nov-22/nutrition_small.csv.gz')"
      ],
      "metadata": {
        "id": "l2BB6BBB7oWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#display the dataset here so we can see what it looks like\n",
        "nutrition_df"
      ],
      "metadata": {
        "id": "G46hsJy3sScQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, there is a vast collection of nutritional information on a large variety of foods. However, there is one big issue - how do we link this information to the prices of the products?"
      ],
      "metadata": {
        "id": "uSgs0prXsji9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Introducing the similarity join\n",
        "\n",
        "If you compare the pricing dataframe and the nutritional dataframe side by side, just by looking at the product names you can see that there are a lot of products that are in both!\n",
        "\n",
        "But, that names are not exactly the same, even though we know they are  referring to the same product.\n",
        "\n",
        "How do we match these products together? We can use a tool called [py-stringsimjoin](http://anhaidgroup.github.io/py_stringsimjoin/v0.3.2/overview.html) for performing a method called a **similarity join**"
      ],
      "metadata": {
        "id": "o8Ki7n1Is_fW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install py-stringsimjoin"
      ],
      "metadata": {
        "id": "ml05Oy7n79mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import py_stringsimjoin as ssj\n",
        "import py_stringmatching as sm"
      ],
      "metadata": {
        "id": "7qy14Ues7-o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####What is a similarity join?\n",
        "At its core, a similarity join takes a look at 2 strings, and gives them a score based on how similar they are to each other.\n",
        "\n",
        "Lets take a look at 2 products -\n",
        "\n",
        "`Coke Can 250ml`\n",
        "\n",
        "`Diet Coke Can 330ml`\n",
        "\n",
        "We can split these names into individual words (sometimes referred to as tokens)\n",
        "\n",
        "`[Coke, Can, 250ml]`\n",
        "\n",
        "`[Diet, Coke, Can, 330ml]`\n",
        "\n",
        "To compute a *Jaccard* similarity score:\n",
        "\n",
        "$\\text{Score} = \\frac{ \\text{Number of shared words} }{ \\text{Total number of unique words}}$\n",
        "\n",
        "In this case, the shared words are:\n",
        "\n",
        "`[Coke, Can]`\n",
        "\n",
        "and all the unique words between the 2 strings are:\n",
        "\n",
        "`[Diet, Coke, Can, 250ml, 330ml]`\n",
        "\n",
        "therefore: \n",
        "\n",
        "$\\text{Score} = \\frac{2}{5} = 0.4$\n",
        "\n",
        "The higher the score is, the more similar the strings are. The lower the score is, the less similar the strings are."
      ],
      "metadata": {
        "id": "wdtkP9HLt_dX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenisation\n",
        "\n",
        "The first thing we need is a method to split strings into individual words/tokens.\n",
        "\n",
        "The variable below contains an [alphanumeric tokenizer](https://anhaidgroup.github.io/py_stringmatching/v0.4.2/AlphanumericTokenizer.html), which takes strings and splits into individual words, ignoring any characters that aren't numbers or letters."
      ],
      "metadata": {
        "id": "wNG0n0mpz7aJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alnum_tok = sm.AlphanumericTokenizer(return_set=True)"
      ],
      "metadata": {
        "id": "H8RIVmNB0Axg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets give this a try:"
      ],
      "metadata": {
        "id": "kgO6wQTb0LVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Diet Coke', alnum_tok.tokenize('Diet Coke'))\n",
        "print('Diet-Coke', alnum_tok.tokenize('Diet-Coke'))"
      ],
      "metadata": {
        "id": "pQV13gCe0Ovf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Before We Join:\n",
        "\n",
        "The goal of joining is to pick the 'best' match from the possible options.\n",
        "If there are multiple matches for the same product, we would want to prioritise:\n",
        "- products that are from the UK\n",
        "- products that have nutritional information filled out.\n",
        "\n",
        "We also want to make sure all the product names are in lowercase."
      ],
      "metadata": {
        "id": "gxkV1wef1yF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nutrition_columns = [\n",
        "    'nutriments.fat_100g',\n",
        "    'nutriments.proteins_100g',\n",
        "    'nutriments.sugars_100g',\n",
        "    'nutriments.salt_100g',\n",
        "    'nutriments.energy-kcal_100g',\n",
        "    'nutriments.carbohydrates_100g'\n",
        "    ]\n",
        "\n",
        "#this is required by the join module.\n",
        "nutrition_df['uid'] = range(0, len(nutrition_df))\n",
        "\n",
        "#add a true/false column to check if the row has nutritional info\n",
        "nutrition_df['has_nutritional_info'] = nutrition_df[nutrition_columns].notna().any(axis='columns')\n",
        "\n",
        "#add a true/false column to check if the product is from the UK\n",
        "nutrition_df['is_uk'] = nutrition_df['countries_tags'].apply(lambda x: ('en:united-kingdom' in eval(x)) if isinstance(x, str) else False)\n",
        "\n",
        "#add a column to both dataframes with the product names in lowercase\n",
        "nutrition_df['product_name_lower'] = nutrition_df['product_name'].apply(lambda x: x.lower())\n",
        "product_df['name_lower'] = product_df['name'].apply(lambda x: x.lower())"
      ],
      "metadata": {
        "id": "_MLYnbm59mdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Perform the join\n",
        "\n",
        "Next, lets perform the join!\n",
        "This function creates a table of all the possible matches for each product, and their similarity score. It is important to pick a threshold/minimum score, so that matches with a score lower than this are not included."
      ],
      "metadata": {
        "id": "7wZSRJxG4HLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to change this to see how it affects the results.\n",
        "similarity_threshold = 0.6\n",
        "\n",
        "output_pairs = ssj.jaccard_join(\n",
        "    product_df, nutrition_df, \n",
        "    'id', 'uid', 'name_lower', 'product_name_lower', \n",
        "    alnum_tok, similarity_threshold, \n",
        "    l_out_attrs=['name'], r_out_attrs=['product_name', 'has_nutritional_info', 'is_uk'])"
      ],
      "metadata": {
        "id": "uSMipbT_9i8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Pick the best Match:\n",
        "\n",
        "The `output_pairs` has multiple results for each product. Next, we want to pick the best match for each product and remove all the duplicates"
      ],
      "metadata": {
        "id": "trQGL3wi4kLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deduped_pairs = output_pairs.sort_values(['_sim_score', 'r_has_nutritional_info', 'r_is_uk'], ascending=False).drop_duplicates(['l_id'])\n",
        "deduped_pairs"
      ],
      "metadata": {
        "id": "_R-5LGFc-Cn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Join the prices with nutrition\n",
        "Now we have the matching table, lets make use of that to join the prices table and nutrition table together! For this, we will use [df.merge](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html), which joins 2 tables together based on their ids."
      ],
      "metadata": {
        "id": "huPd5C5F4-f_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#join the prices with the pairs table\n",
        "combined_df = price_df.merge(deduped_pairs,left_on='id',right_on='l_id')\n",
        "\n",
        "#join the combined table with the nutrition table\n",
        "combined_df = combined_df.merge(nutrition_df,left_on='r_uid',right_on='uid')\n",
        "combined_df"
      ],
      "metadata": {
        "id": "IFimzYRR-YWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Looking at nutrition and prices\n",
        "\n",
        "Next, lets use our new table to find out what the cheapest products are for the nutritional value they provide!\n",
        "\n",
        "First, lets increase the amount of data we can see to better see our results:"
      ],
      "metadata": {
        "id": "dq8vOY82Ekx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.max_rows', 100)"
      ],
      "metadata": {
        "id": "ohVUzlFuFTpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we should filter the combined table to remove missing data, and pick only the most nutritious food. \n",
        "\n",
        "We will also need to make sure both the price and nutrition are using the same units (per 100g)."
      ],
      "metadata": {
        "id": "HUuwiVTdFrsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_filter = (\n",
        "    combined_df['price_unit_amount'] == 100\n",
        "    ) & (combined_df['nutriments.energy-kcal_100g'].notna()\n",
        "    ) & (combined_df['nutriments.energy-kcal_100g'] > 0\n",
        "    ) & (combined_df['price_per_unit'] > 0\n",
        "    ) & (combined_df['nutriscore_grade'].isin(['a','b'])\n",
        ")\n",
        "\n",
        "df_to_analyse = combined_df[combined_filter].copy(deep=True)"
      ],
      "metadata": {
        "id": "bQgv4QUCFq1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let create a new column to work out the cost for a full days worth of calories from each product, and use it to find the cheapest products"
      ],
      "metadata": {
        "id": "hHe1Is2eFWec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Work out the price per 2000 calories for each product\n",
        "df_to_analyse['price_per_2000kcal'] = (df_to_analyse['price_per_unit'] / df_to_analyse['nutriments.energy-kcal_100g']) * 2000\n",
        "df_to_analyse.sort_values('price_per_2000kcal').head(100)"
      ],
      "metadata": {
        "id": "aRbILJimAzvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now also have a look at the other nutrition columns to see if other foods are better for other types of nutrition."
      ],
      "metadata": {
        "id": "pZ7ijXNsGX5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_to_analyse['price_per_50g_protein'] = (df_to_analyse['price_per_unit'] / df_to_analyse['nutriments.proteins_100g']) * 50\n",
        "df_to_analyse.sort_values('price_per_50g_protein').head(100)"
      ],
      "metadata": {
        "id": "ej4ahHC9FU-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do you think of the results you've found so far? Would you actually change what you eat based on it?"
      ],
      "metadata": {
        "id": "bsZrWhD0UQuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4 (Optional) - Categorising the products"
      ],
      "metadata": {
        "id": "2Z2OGrSKaVhY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rather than forcing people to change what they eat, an alternative approach to saving money is picking alternative items for whats in their basket.\n",
        "\n",
        "For example, most supermarkets have their own brand of milk, which only they sell:"
      ],
      "metadata": {
        "id": "q940TeiaOdjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "product_df[product_df['id'] == 43060]"
      ],
      "metadata": {
        "id": "bPtWHR7oSg1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This makes it difficult to directly compare Sainsbury's milk with Tesco's milk for example, using our current prices dataframe.\n",
        "\n",
        "It also makes it difficult to compare different quantities (e.g. 4 pints of milk vs 6 pints of milk), to find what is actually cheaper.\n",
        "\n",
        "###Using Categories\n",
        "\n",
        "However, if we take a look at the nutrition dataframe, we can see that in the field `categories_hierarchy`, we do have categories!\n"
      ],
      "metadata": {
        "id": "ZBCLLd4PTE9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nutrition_df[nutrition_df['categories_hierarchy'].notna()].head(5)"
      ],
      "metadata": {
        "id": "ZC-VahQhXrtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our biggest problem here, however, is that a lot of the products from the products data frame don't have a good match to a product in the nutrition dataframe.\n",
        "\n",
        "If only we had a way to take a look at a product's name and figure out what categories it belongs to...\n",
        "\n",
        "###Introducing Classification/Labelling\n",
        "\n",
        "Using the power of machine learning, we can build a model that learns from a dataset of product names, and the categories they belong to, and uses it to predict the categories of new product names!\n",
        "\n",
        "###Data Preparation\n",
        "\n",
        "The first thing we need to do is to clean up and prepare the data so that its ready to be used by a model. The first step here is to turn the categories from a [string](https://docs.python.org/3/library/stdtypes.html#textseq) to a [list](https://docs.python.org/3/library/stdtypes.html#lists) - this will make it easier to use later on, as it makes it easier to access the individual categories for each product."
      ],
      "metadata": {
        "id": "y2Z_t0lAYFyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The categories are currently saved as a string - this turns them into a list\n",
        "nutrition_df['categories_list'] = nutrition_df['categories_hierarchy'].apply(lambda x: eval(x) if isinstance(x, str) else [])"
      ],
      "metadata": {
        "id": "P8yy8NZRjdyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next thing we want to do is remove any non english categories, and any products without categories - its important to keep the data we use to only useful information - 'bad' rows will affect our results."
      ],
      "metadata": {
        "id": "WOhI_8RNZ0uJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We only want to focus on products with english categories - this removes non english categories, and then removes products without categories\n",
        "en_nutrition_df = nutrition_df[nutrition_df['categories_list'].apply(lambda x: (len(x) > 0) and len([not_en for not_en in x if not_en.split(':')[0] != 'en']) == 0)].copy()"
      ],
      "metadata": {
        "id": "lWzzI2H_aq2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a model to learn how to categorize products correctly, it will need lots of examples to learn from! \n",
        "\n",
        "If it doesn't have enough examples for each category, it won't be able to correctly identify that category, and that will affect the other results as well.\n",
        "\n",
        "The next stage is to figure out which categories to use by counting how many examples we have for each one, and creating a list of categories to use, based on a minimum count. Try experimenting the the minimum count to see how it affects results."
      ],
      "metadata": {
        "id": "NrB7zEAHaspc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the minimum number of examples there has to be of each category for it to be included in our model.\n",
        "minimum_count_threshold = 200\n",
        "\n",
        "# There are some terms with occurrence as low as 1.\n",
        "terms_list = []\n",
        "for category_list in en_nutrition_df['categories_list']:\n",
        "  for c in category_list:\n",
        "    terms_list.append(c)\n",
        "len(terms_list)\n",
        "\n",
        "categories_df = pd.DataFrame(terms_list, columns=['category'])\n",
        "column_counts = categories_df.groupby('category').size().reset_index(name='count')\n",
        "categories_to_use = column_counts[column_counts['count'] > minimum_count_threshold]\n",
        "categories_to_use.values"
      ],
      "metadata": {
        "id": "oJH9aCfKaS2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training purposes, we want to make sure that there are atleast 2 examples of each combination of categories - this will explained later in more detail - this is used for the train test split.\n",
        "\n",
        "To do this, we need to make a list of combinations to drop."
      ],
      "metadata": {
        "id": "HGDJ07lkcdSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This creates a list of combinations to drop.\n",
        "hierarchy_df = en_nutrition_df.groupby('categories_hierarchy').size().reset_index(name='count')\n",
        "categories_to_drop = hierarchy_df[hierarchy_df['count'] < 2]\n",
        "categories_to_drop"
      ],
      "metadata": {
        "id": "loIc-5REtTZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can apply the `categories_to_use` and `categories_to_drop` to filter out any unwanted data, and again remove any rows that have no categories, leaving us with a labelled dataset ready for machine learning!"
      ],
      "metadata": {
        "id": "4yYcPPZTdh03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en_nutrition_df['train_categories'] = en_nutrition_df['categories_list'].apply(lambda categories: [c for c in categories if c in categories_to_use['category'].values])\n",
        "label_filter = (\n",
        "    en_nutrition_df['train_categories'].apply(lambda x: len(x) > 0)\n",
        "    ) & (~en_nutrition_df['categories_hierarchy'].isin(categories_to_drop['categories_hierarchy']))\n",
        "labelled_nutrition_df = en_nutrition_df[label_filter]\n",
        "len(labelled_nutrition_df)"
      ],
      "metadata": {
        "id": "94HdViDBpgZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Building the model\n",
        "\n",
        "Now we have our data, we can use it to build and train a model. The design we are going to use generally follows [this tutorial.](https://keras.io/examples/nlp/multi_label_classification/). Please refer to the tutorial if you require more explanation on concepts - we will largely focus on using this design to get results \n",
        "\n",
        "The imports below largely come from [tensorflow](https://www.tensorflow.org/) and [scikit-learn](https://scikit-learn.org/), two common machine learning libraries."
      ],
      "metadata": {
        "id": "gVuWJMOJeD7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from ast import literal_eval\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "19Oi4T31iCB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Splitting the data\n",
        "\n",
        "The first task we need to do is split the data into a training dataset and test dataset.\n",
        "\n",
        "The training dataset will be used by the model for learning, whilst the test dataset will be used to see how well the model is performing.\n",
        "\n",
        "This is the reason why we needed atleast 2 examples of each category combination, so we can have atleast one in the training set, and one in the test set, to make it a fair test."
      ],
      "metadata": {
        "id": "kUYet7iOk6xR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this means 90% of the data will be used for training, and 10% of the data will be used for testing\n",
        "test_split = 0.1\n",
        "\n",
        "# Initial train and test split.\n",
        "train_df, test_df = train_test_split(\n",
        "    labelled_nutrition_df,\n",
        "    test_size=test_split,\n",
        "    stratify=labelled_nutrition_df[\"categories_hierarchy\"].values,\n",
        ")\n",
        "\n",
        "# Splitting the test set further into validation\n",
        "# and new test sets.\n",
        "val_df = test_df.sample(frac=0.5)\n",
        "test_df.drop(val_df.index, inplace=True)\n",
        "\n",
        "print(f\"Number of rows in training set: {len(train_df)}\")\n",
        "print(f\"Number of rows in validation set: {len(val_df)}\")\n",
        "print(f\"Number of rows in test set: {len(test_df)}\")"
      ],
      "metadata": {
        "id": "4txfxrdihwFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####String Preprocessing\n",
        "With Tensorflow, and deep learning models in general, all the data going into the model needs to be represented as numbers, or vectors/arrays of numbers.\n",
        "\n",
        "The following cell provides functionality to convert the **product name** and **categories** to a number representation."
      ],
      "metadata": {
        "id": "KYPNr_MAFkrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "terms = tf.ragged.constant(categories_to_use['category'].values)\n",
        "lookup = tf.keras.layers.StringLookup(output_mode=\"multi_hot\")\n",
        "lookup.adapt(terms)\n",
        "vocab = lookup.get_vocabulary()\n",
        "\n",
        "\n",
        "def invert_multi_hot(encoded_labels):\n",
        "    \"\"\"Reverse a single multi-hot encoded label to a tuple of vocab terms.\"\"\"\n",
        "    hot_indices = np.argwhere(encoded_labels == 1.0)[..., 0]\n",
        "    return np.take(vocab, hot_indices)\n",
        "\n",
        "max_seqlen = 20\n",
        "batch_size = 128\n",
        "padding_token = \"<pad>\"\n",
        "auto = tf.data.AUTOTUNE\n",
        "\n",
        "\n",
        "def make_dataset(dataframe, is_train=True):\n",
        "    labels = tf.ragged.constant(dataframe[\"train_categories\"].values)\n",
        "    label_binarized = lookup(labels).numpy()\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (dataframe[\"product_name_lower\"].values, label_binarized)\n",
        "    )\n",
        "    dataset = dataset.shuffle(batch_size * 10) if is_train else dataset\n",
        "    return dataset.batch(batch_size)\n",
        "\n",
        "train_dataset = make_dataset(train_df, is_train=True)\n",
        "validation_dataset = make_dataset(val_df, is_train=False)\n",
        "test_dataset = make_dataset(test_df, is_train=False)\n",
        "\n",
        "vocabulary = set()\n",
        "train_df[\"product_name_lower\"].str.lower().str.split().apply(vocabulary.update)\n",
        "vocabulary_size = len(vocabulary)\n",
        "\n",
        "text_vectorizer = layers.TextVectorization(\n",
        "    max_tokens=vocabulary_size, ngrams=3, output_mode=\"tf_idf\"\n",
        ")\n",
        "\n",
        "# `TextVectorization` layer needs to be adapted as per the vocabulary from our\n",
        "# training set.\n",
        "with tf.device(\"/CPU:0\"):\n",
        "    text_vectorizer.adapt(train_dataset.map(lambda text, label: text))\n",
        "\n",
        "train_dataset = train_dataset.map(\n",
        "    lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto\n",
        ").prefetch(auto)\n",
        "validation_dataset = validation_dataset.map(\n",
        "    lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto\n",
        ").prefetch(auto)\n",
        "test_dataset = test_dataset.map(\n",
        "    lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto\n",
        ").prefetch(auto)"
      ],
      "metadata": {
        "id": "JkOAC-t-sZg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next function returns a model that we can feed data to, and train."
      ],
      "metadata": {
        "id": "QYu9tkyIIa0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model():\n",
        "    shallow_mlp_model = keras.Sequential(\n",
        "        [\n",
        "            layers.Dense(1024, activation=\"relu\"),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Dense(512, activation=\"relu\"),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Dense(lookup.vocabulary_size(), activation=\"sigmoid\"),\n",
        "        ]\n",
        "    )\n",
        "    return shallow_mlp_model"
      ],
      "metadata": {
        "id": "autpRegGulO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can train the model, using `model.fit`, to learn the categories from the product names.\n",
        "\n",
        "The `top_k_categorical_accuracy` metric gives a rough idea of how accurate the model is at predicting the right categories."
      ],
      "metadata": {
        "id": "eGogWXpHIqnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "\n",
        "shallow_mlp_model = make_model()\n",
        "shallow_mlp_model.compile(\n",
        "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"top_k_categorical_accuracy\"]\n",
        ")\n",
        "\n",
        "history = shallow_mlp_model.fit(\n",
        "    train_dataset, validation_data=validation_dataset, epochs=epochs\n",
        ")\n",
        "\n",
        "\n",
        "def plot_result(item):\n",
        "    plt.plot(history.history[item], label=item)\n",
        "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(item)\n",
        "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_result(\"loss\")\n",
        "plot_result(\"top_k_categorical_accuracy\")"
      ],
      "metadata": {
        "id": "gu0q6kMgurqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the test data we saved previously, we can check how accurate this model is, on data it hasn't seen before."
      ],
      "metadata": {
        "id": "AO2FN0JOJPwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, categorical_acc = shallow_mlp_model.evaluate(test_dataset)\n"
      ],
      "metadata": {
        "id": "o1a9Hv-tycDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Using the model\n",
        "\n",
        "Now that we have our model ready and trained, lets use it to predict the categories of all the products in the product dataframe."
      ],
      "metadata": {
        "id": "esVsnxoYJiZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model for inference.\n",
        "model_for_inference = keras.Sequential([text_vectorizer, shallow_mlp_model])\n",
        "\n",
        "predicted_probabilities = model_for_inference.predict(product_df['name_lower'].values)\n",
        "\n",
        "# Perform inference.\n",
        "\n",
        "predicted_labels = []\n",
        "for i, predicted_proba in enumerate(predicted_probabilities):\n",
        "    top_5_labels = [\n",
        "        x\n",
        "        for p, x in sorted(\n",
        "            zip(predicted_proba, lookup.get_vocabulary()),\n",
        "            key=lambda pair: pair[0],\n",
        "            reverse=True,\n",
        "        ) if p > 0.8\n",
        "    ][:5]\n",
        "    if i % 1000 == 0:\n",
        "      print('processed {} predictions'.format(i))\n",
        "    predicted_labels.append(top_5_labels)\n",
        "\n",
        "product_df['predicted_labels'] = predicted_labels"
      ],
      "metadata": {
        "id": "CWRm1z7DEEzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the predicted categories, lets use them to compare products! First, pick a category to use, and find the cheapest product in that category, per 100ml or 100g of product!"
      ],
      "metadata": {
        "id": "dZFZLiWgKB4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Filter the products to pick only ones that match a category\n",
        "bread_products = product_df[product_df['predicted_labels'].apply(lambda x: 'en:milks' in x)]\n",
        "\n",
        "# Merge the products with the prices\n",
        "bread_prices = bread_products.merge(price_df,left_on='id',right_on='id')\n",
        "\n",
        "# Sort the prices to find the cheapest products in that category\n",
        "bread_prices.sort_values('price_per_unit').head(100)"
      ],
      "metadata": {
        "id": "fPO2BQ8_JS-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do think of the results? Are there products in the wrong category? Why might that be?"
      ],
      "metadata": {
        "id": "WOzYwVHoKVZ6"
      }
    }
  ]
}